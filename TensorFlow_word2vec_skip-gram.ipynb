{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import urllib\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "num_steps = 3000000\n",
    "display_step = 10000\n",
    "eval_step = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation parameters\n",
    "eval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woed2Vec parameters\n",
    "embedding_size = 200\n",
    "max_vocabulary_size = 5000 \n",
    "min_occurrence = 10 # Remove all words that does not appears at least n times\n",
    "skip_window = 3 # How many words to consider left and right\n",
    "num_skips = 2 # How many times to reuse an input to generate a label 从整个窗口中选取多少个不同的词\n",
    "num_sampled = 64 # Number of negative examples to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "# data_path = 'text8.zip'\n",
    "# if not os.path.exists(data_path):\n",
    "#     print(\"Downloading the dataset... (It may take some time)\")\n",
    "#     filename, _ = urllib.request.urlretrieve(url, data_path)\n",
    "#     print(\"Done!\")\n",
    "# Unzip the dataset file. Text has already been processed\n",
    "data_path = 'C:/Users/Crow/Desktop/text8.zip'\n",
    "with zipfile.ZipFile(data_path) as f:\n",
    "    text_words = f.read(f.namelist()[0]).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'anarc'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_words[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count: 17005207\n",
      "Unique words: 253854\n",
      "Vocabulary size: 5000\n",
      "Most common words: [('UNK', 2735459), (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764), (b'in', 372201), (b'a', 325873), (b'to', 316376), (b'zero', 264975), (b'nine', 250430)]\n"
     ]
    }
   ],
   "source": [
    "# Build the dictionary and replace rare words with UNK token\n",
    "count = [('UNK', -1)]\n",
    "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n",
    "for i in range(len(count) - 1,-1,-1):\n",
    "    if count[i][1] < min_occurrence:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "vocabulary_size = len(count)\n",
    "word2id = dict()\n",
    "for i, (word, _) in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "for word in text_words:\n",
    "    index = word2id.get(word, 0)\n",
    "    if index == 0:\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0] = ('UNK', unk_count)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "print(\"Words count:\", len(text_words))\n",
    "print(\"Unique words:\", len(set(text_words)))\n",
    "print(\"Vocabulary size:\", vocabulary_size)\n",
    "print(\"Most common words:\", count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# generate training batch for the skip-gram model\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index # 为定义在函数外的变量赋值\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch  = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # get window size (words left and right + current one)\n",
    "    span = 2 * skip_window + 1\n",
    "    # 设置缓冲区最大程度保留span元素，是一种用于采样的移动窗口\n",
    "    buffer = collections.deque(maxlen=span) # double-ended queue 双边队列 \n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, shape=[None]) # input data\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1]) # input label\n",
    "\n",
    "# ensure the following ops & var are assigned on cpu\n",
    "# 有些操作在GPU上不兼容\n",
    "with tf.device('/cpu:0'):\n",
    "    # create the embedding variable (each row represent a word embedding vector)\n",
    "    embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
    "    # lookup the corresponding embedding vectors for each sample in X\n",
    "    X_embed = tf.nn.embedding_lookup(embedding, X)\n",
    "    \n",
    "    # construct the variables for the NCE loss\n",
    "    # NCE（Noise Contrastive Estimation，噪声对比估计，的速度更快，可以作为替代方案。这个方法不是用上下文单词相对于词汇表中所有可能的上下文单词的概率，而是随机抽样 2-20 个可能的上下文单词，并仅从这些单词中评估概率。该方法可用于训练模型，且可大大加快训练进程\n",
    "    nce_weight = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "# compute the average NCE loss for the batch\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weight,\n",
    "                   biases=nce_biases,\n",
    "                   labels=Y,\n",
    "                   inputs=X_embed,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocabulary_size\n",
    "                  )\n",
    ")\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# evaluation\n",
    "# compute the cosine similarity between input data embedding and every embedding vectors\n",
    "X_embed_norm = X_embed / tf.sqrt(tf.reduce_sum(tf.square(X_embed)))\n",
    "embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "cosine_sim_op = tf.matmul(X_embed_norm, embedding_norm, transpose_b=True) # transpose_b: 如果为真, b则在进行乘法计算前进行转置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, average loss = 466.9443\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'van', b'frequency', b'kilometers', b'career', b'commission', b'benjamin', b'literally', b'captain',\n",
      "\"b'of'\" nearest neighbors: b'yards', b'group', b'be', b'evolved', b'member', b'carrying', b'supposed', b'california',\n",
      "\"b'going'\" nearest neighbors: b'charge', b'cook', b'chose', b'python', b'advances', b'dollar', b'stanford', b'attempted',\n",
      "\"b'hardware'\" nearest neighbors: b'larry', b'single', b'operations', b'african', b'greatly', b'cutting', b'appearances', b'iron',\n",
      "\"b'american'\" nearest neighbors: b'fighting', b'ex', b'combat', b'driver', b'addresses', b'pope', b'husband', b'kennedy',\n",
      "\"b'britain'\" nearest neighbors: b'affect', b'phenomenon', b'velocity', b'controls', b'effort', b'hungarian', b'simple', b'swiss',\n",
      "step 10000, average loss = 72.5968\n",
      "step 20000, average loss = 18.7944\n",
      "step 30000, average loss = 12.3687\n",
      "step 40000, average loss = 10.1661\n",
      "step 50000, average loss = 8.7488\n",
      "step 60000, average loss = 8.0689\n",
      "step 70000, average loss = 7.9489\n",
      "step 80000, average loss = 7.4694\n",
      "step 90000, average loss = 7.2259\n",
      "step 100000, average loss = 7.0544\n",
      "step 110000, average loss = 6.8499\n",
      "step 120000, average loss = 6.8631\n",
      "step 130000, average loss = 6.6472\n",
      "step 140000, average loss = 6.6338\n",
      "step 150000, average loss = 6.6229\n",
      "step 160000, average loss = 6.4622\n",
      "step 170000, average loss = 6.4388\n",
      "step 180000, average loss = 6.3405\n",
      "step 190000, average loss = 6.4064\n",
      "step 200000, average loss = 6.2107\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'three', b'four', b'six', b'seven', b'two', b'eight', b'nine', b'zero',\n",
      "\"b'of'\" nearest neighbors: b'the', b'in', b'and', b'from', b'on', b'a', b's', b'by',\n",
      "\"b'going'\" nearest neighbors: b'on', b'in', b'from', b'which', b'with', b'by', b'the', b'or',\n",
      "\"b'hardware'\" nearest neighbors: b'this', b'not', b'or', b'to', b'but', b'which', b'larry', b'is',\n",
      "\"b'american'\" nearest neighbors: UNK, b'and', b'one', b'nine', b'in', b'seven', b'six', b'four',\n",
      "\"b'britain'\" nearest neighbors: b'are', b'for', b'at', b'were', b'is', b'as', b'all', b'had',\n",
      "step 210000, average loss = 5.9470\n",
      "step 220000, average loss = 6.0784\n",
      "step 230000, average loss = 6.2159\n",
      "step 240000, average loss = 6.1235\n",
      "step 250000, average loss = 6.0309\n",
      "step 260000, average loss = 5.9978\n",
      "step 270000, average loss = 6.0619\n",
      "step 280000, average loss = 5.8941\n",
      "step 290000, average loss = 5.7896\n",
      "step 300000, average loss = 5.8931\n",
      "step 310000, average loss = 5.9298\n",
      "step 320000, average loss = 5.8657\n",
      "step 330000, average loss = 5.9696\n",
      "step 340000, average loss = 5.8987\n",
      "step 350000, average loss = 5.8270\n",
      "step 360000, average loss = 5.8578\n",
      "step 370000, average loss = 5.7809\n",
      "step 380000, average loss = 5.7827\n",
      "step 390000, average loss = 5.7416\n",
      "step 400000, average loss = 5.7462\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'nine',\n",
      "\"b'of'\" nearest neighbors: b'and', b'the', b'for', b'in', b'with', b'from', UNK, b'a',\n",
      "\"b'going'\" nearest neighbors: b'on', b'through', b'after', b'more', b'in', b'its', b'only', b'was',\n",
      "\"b'hardware'\" nearest neighbors: b'a', b'and', b'single', b'for', b'only', UNK, b'which', b'larry',\n",
      "\"b'american'\" nearest neighbors: b's', b'in', b'one', b'eight', b'seven', b'and', b'from', b'six',\n",
      "\"b'britain'\" nearest neighbors: b'time', b'at', b'were', b'for', b'all', b'with', b'its', b'most',\n",
      "step 410000, average loss = 5.6814\n",
      "step 420000, average loss = 5.7308\n",
      "step 430000, average loss = 5.6553\n",
      "step 440000, average loss = 5.6993\n",
      "step 450000, average loss = 5.6291\n",
      "step 460000, average loss = 5.6930\n",
      "step 470000, average loss = 5.4031\n",
      "step 480000, average loss = 5.5274\n",
      "step 490000, average loss = 5.5671\n",
      "step 500000, average loss = 5.5455\n",
      "step 510000, average loss = 5.5666\n",
      "step 520000, average loss = 5.5413\n",
      "step 530000, average loss = 5.4829\n",
      "step 540000, average loss = 5.5525\n",
      "step 550000, average loss = 5.3219\n",
      "step 560000, average loss = 5.4603\n",
      "step 570000, average loss = 5.5018\n",
      "step 580000, average loss = 5.4973\n",
      "step 590000, average loss = 5.4445\n",
      "step 600000, average loss = 5.5466\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'three', b'six', b'seven', b'two', b'eight', b'one', b'zero',\n",
      "\"b'of'\" nearest neighbors: b'and', b'the', b'in', b'its', b'for', b'with', b'or', b'from',\n",
      "\"b'going'\" nearest neighbors: b'through', b'on', b'century', b'second', b'more', b'when', b'over', b'after',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'another', b'larry', b'now', b'a', b'under', b'operations', b'called',\n",
      "\"b'american'\" nearest neighbors: b'english', b'b', b's', b'd', b'from', b'in', b'seven', b'eight',\n",
      "\"b'britain'\" nearest neighbors: b'both', b'number', b'over', b'war', b'at', b'use', b'high', b'city',\n",
      "step 610000, average loss = 5.4500\n",
      "step 620000, average loss = 5.4536\n",
      "step 630000, average loss = 5.4690\n",
      "step 640000, average loss = 5.3983\n",
      "step 650000, average loss = 5.4527\n",
      "step 660000, average loss = 5.3648\n",
      "step 670000, average loss = 5.3850\n",
      "step 680000, average loss = 5.3768\n",
      "step 690000, average loss = 5.3740\n",
      "step 700000, average loss = 5.3827\n",
      "step 710000, average loss = 5.3588\n",
      "step 720000, average loss = 5.3935\n",
      "step 730000, average loss = 5.3594\n",
      "step 740000, average loss = 5.0572\n",
      "step 750000, average loss = 5.2733\n",
      "step 760000, average loss = 5.3336\n",
      "step 770000, average loss = 5.2976\n",
      "step 780000, average loss = 5.2648\n",
      "step 790000, average loss = 5.2675\n",
      "step 800000, average loss = 5.3046\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'seven', b'three', b'eight', b'two', b'nine', b'zero',\n",
      "\"b'of'\" nearest neighbors: b'and', b'the', b'for', b'in', b'while', b'its', b'with', b'from',\n",
      "\"b'going'\" nearest neighbors: b'through', b'century', b'more', b'second', b'another', b'over', b'back', b'due',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'another', b'larry', b'where', b'this', b'a', b'now', b'even',\n",
      "\"b'american'\" nearest neighbors: b'b', b'english', b'british', b'd', b'nine', b'eight', b's', b'seven',\n",
      "\"b'britain'\" nearest neighbors: b'ii', b'part', b'city', b'name', b'days', b'high', b'time', b'government',\n",
      "step 810000, average loss = 5.2287\n",
      "step 820000, average loss = 5.1492\n",
      "step 830000, average loss = 5.2284\n",
      "step 840000, average loss = 5.2715\n",
      "step 850000, average loss = 5.2559\n",
      "step 860000, average loss = 5.2952\n",
      "step 870000, average loss = 5.2696\n",
      "step 880000, average loss = 5.2228\n",
      "step 890000, average loss = 5.2639\n",
      "step 900000, average loss = 5.2126\n",
      "step 910000, average loss = 5.2280\n",
      "step 920000, average loss = 5.2195\n",
      "step 930000, average loss = 5.1925\n",
      "step 940000, average loss = 5.1541\n",
      "step 950000, average loss = 5.2203\n",
      "step 960000, average loss = 5.1776\n",
      "step 970000, average loss = 5.2201\n",
      "step 980000, average loss = 5.1619\n",
      "step 990000, average loss = 5.2113\n",
      "step 1000000, average loss = 5.0160\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'nine', b'zero',\n",
      "\"b'of'\" nearest neighbors: b'and', b'in', b'the', b'from', b'first', b's', b'including', b'by',\n",
      "\"b'going'\" nearest neighbors: b'back', b'due', b'game', b'century', b'up', b'chose', b'become', b'through',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'left', b'larry', b'another', b'certain', b'where', b'operations', b'now',\n",
      "\"b'american'\" nearest neighbors: b'b', b'd', b'nine', b'english', b'eight', b'british', b'seven', b'and',\n",
      "\"b'britain'\" nearest neighbors: b'ii', b'part', b'days', b'city', b'party', b'country', b'various', b'built',\n",
      "step 1010000, average loss = 5.0556\n",
      "step 1020000, average loss = 5.1527\n",
      "step 1030000, average loss = 5.1222\n",
      "step 1040000, average loss = 5.1400\n",
      "step 1050000, average loss = 5.1042\n",
      "step 1060000, average loss = 5.0984\n",
      "step 1070000, average loss = 5.1780\n",
      "step 1080000, average loss = 4.9948\n",
      "step 1090000, average loss = 5.0936\n",
      "step 1100000, average loss = 5.1157\n",
      "step 1110000, average loss = 5.1217\n",
      "step 1120000, average loss = 5.1090\n",
      "step 1130000, average loss = 5.1422\n",
      "step 1140000, average loss = 5.1334\n",
      "step 1150000, average loss = 5.0759\n",
      "step 1160000, average loss = 5.1209\n",
      "step 1170000, average loss = 5.0685\n",
      "step 1180000, average loss = 5.1093\n",
      "step 1190000, average loss = 5.0684\n",
      "step 1200000, average loss = 5.0663\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'zero', b'nine',\n",
      "\"b'of'\" nearest neighbors: b'including', b'from', b'and', b'the', b'for', b'in', b'while', b'with',\n",
      "\"b'going'\" nearest neighbors: b'back', b'due', b'game', b'become', b'chose', b'control', b'century', b'least',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'certain', b'left', b'larry', b'computer', b'operations', b'then', b'greatly',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'd', b'b', b'english', b'john', b'nine', b'german',\n",
      "\"b'britain'\" nearest neighbors: b'ii', b'days', b'part', b'country', b'city', b'party', b'members', b'built',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1210000, average loss = 5.0358\n",
      "step 1220000, average loss = 5.0789\n",
      "step 1230000, average loss = 5.0827\n",
      "step 1240000, average loss = 5.0771\n",
      "step 1250000, average loss = 5.0730\n",
      "step 1260000, average loss = 5.0807\n",
      "step 1270000, average loss = 4.8143\n",
      "step 1280000, average loss = 5.0216\n",
      "step 1290000, average loss = 5.0518\n",
      "step 1300000, average loss = 5.0147\n",
      "step 1310000, average loss = 5.0034\n",
      "step 1320000, average loss = 5.0094\n",
      "step 1330000, average loss = 5.0193\n",
      "step 1340000, average loss = 5.0213\n",
      "step 1350000, average loss = 4.9194\n",
      "step 1360000, average loss = 4.9762\n",
      "step 1370000, average loss = 5.0228\n",
      "step 1380000, average loss = 5.0331\n",
      "step 1390000, average loss = 5.0350\n",
      "step 1400000, average loss = 5.0200\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'zero', b'one',\n",
      "\"b'of'\" nearest neighbors: b'and', b'for', b'in', b'including', b'with', b'from', UNK, b'while',\n",
      "\"b'going'\" nearest neighbors: b'back', b'due', b'game', b'just', b'become', b'chose', b'open', b'least',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'left', b'certain', b'larry', b'computer', b'operations', b'having', b'back',\n",
      "\"b'american'\" nearest neighbors: b'french', b'english', b'british', b'd', b'german', b'john', b'b', b's',\n",
      "\"b'britain'\" nearest neighbors: b'ii', b'days', b'members', b'country', b'built', b'part', b'countries', b'city',\n",
      "step 1410000, average loss = 4.9966\n",
      "step 1420000, average loss = 5.0245\n",
      "step 1430000, average loss = 5.0101\n",
      "step 1440000, average loss = 4.9851\n",
      "step 1450000, average loss = 5.0101\n",
      "step 1460000, average loss = 4.9769\n",
      "step 1470000, average loss = 4.9309\n",
      "step 1480000, average loss = 4.9976\n",
      "step 1490000, average loss = 4.9831\n",
      "step 1500000, average loss = 5.0052\n",
      "step 1510000, average loss = 4.9658\n",
      "step 1520000, average loss = 4.9978\n",
      "step 1530000, average loss = 4.8648\n",
      "step 1540000, average loss = 4.8515\n",
      "step 1550000, average loss = 4.9473\n",
      "step 1560000, average loss = 4.9439\n",
      "step 1570000, average loss = 4.9595\n",
      "step 1580000, average loss = 4.8974\n",
      "step 1590000, average loss = 4.9102\n",
      "step 1600000, average loss = 4.9871\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'one',\n",
      "\"b'of'\" nearest neighbors: b'and', b'in', b'with', b'for', b'including', b'within', b'through', b'from',\n",
      "\"b'going'\" nearest neighbors: b'back', b'due', b'just', b'open', b'least', b'game', b'chose', b'charge',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'certain', b'computer', b'back', b'space', b'operations', b'left', b'larry',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'english', b'b', b'german', b'd', b'john', b's',\n",
      "\"b'britain'\" nearest neighbors: b'days', b'ii', b'members', b'country', b'city', b'empire', b'built', b'countries',\n",
      "step 1610000, average loss = 4.8667\n",
      "step 1620000, average loss = 4.8992\n",
      "step 1630000, average loss = 4.9404\n",
      "step 1640000, average loss = 4.9492\n",
      "step 1650000, average loss = 4.9390\n",
      "step 1660000, average loss = 4.9616\n",
      "step 1670000, average loss = 4.9590\n",
      "step 1680000, average loss = 4.9136\n",
      "step 1690000, average loss = 4.9598\n",
      "step 1700000, average loss = 4.9058\n",
      "step 1710000, average loss = 4.9371\n",
      "step 1720000, average loss = 4.9303\n",
      "step 1730000, average loss = 4.8957\n",
      "step 1740000, average loss = 4.8672\n",
      "step 1750000, average loss = 4.9268\n",
      "step 1760000, average loss = 4.9199\n",
      "step 1770000, average loss = 4.9292\n",
      "step 1780000, average loss = 4.9111\n",
      "step 1790000, average loss = 4.9339\n",
      "step 1800000, average loss = 4.7024\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'zero', b'nine',\n",
      "\"b'of'\" nearest neighbors: b'in', b'from', b'and', b'following', b'under', b'for', b'while', b'first',\n",
      "\"b'going'\" nearest neighbors: b'back', b'least', b'just', b'open', b'due', b'again', b'chose', b'gave',\n",
      "\"b'hardware'\" nearest neighbors: b'single', b'back', b'computer', b'space', b'operations', b'previous', b'z', b'having',\n",
      "\"b'american'\" nearest neighbors: b'd', b'b', b'british', b'french', b'nine', b'english', b'and', b'eight',\n",
      "\"b'britain'\" nearest neighbors: b'days', b'city', b'empire', b'ii', b'countries', b'members', b'france', b'part',\n",
      "step 1810000, average loss = 4.8758\n",
      "step 1820000, average loss = 4.8930\n",
      "step 1830000, average loss = 4.8626\n",
      "step 1840000, average loss = 4.8637\n",
      "step 1850000, average loss = 4.8778\n",
      "step 1860000, average loss = 4.8668\n",
      "step 1870000, average loss = 4.8940\n",
      "step 1880000, average loss = 4.7876\n",
      "step 1890000, average loss = 4.8557\n",
      "step 1900000, average loss = 4.8957\n",
      "step 1910000, average loss = 4.8994\n",
      "step 1920000, average loss = 4.8889\n",
      "step 1930000, average loss = 4.8964\n",
      "step 1940000, average loss = 4.8673\n",
      "step 1950000, average loss = 4.8891\n",
      "step 1960000, average loss = 4.8881\n",
      "step 1970000, average loss = 4.8554\n",
      "step 1980000, average loss = 4.8876\n",
      "step 1990000, average loss = 4.8564\n",
      "step 2000000, average loss = 4.8485\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'nine',\n",
      "\"b'of'\" nearest neighbors: b'for', b'in', b'following', b'within', b'while', b'with', b'including', b'from',\n",
      "\"b'going'\" nearest neighbors: b'back', b'just', b'again', b'least', b'open', b'gave', b'game', b'chose',\n",
      "\"b'hardware'\" nearest neighbors: b'computer', b'previous', b'software', b'z', b'back', b'space', b'operations', b'having',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'd', b'german', b'b', b'english', b'nine', b'john',\n",
      "\"b'britain'\" nearest neighbors: b'days', b'france', b'empire', b'city', b'country', b'countries', b'germany', b'soviet',\n",
      "step 2010000, average loss = 4.8366\n",
      "step 2020000, average loss = 4.8593\n",
      "step 2030000, average loss = 4.8775\n",
      "step 2040000, average loss = 4.8618\n",
      "step 2050000, average loss = 4.8826\n",
      "step 2060000, average loss = 4.7995\n",
      "step 2070000, average loss = 4.6994\n",
      "step 2080000, average loss = 4.8216\n",
      "step 2090000, average loss = 4.8578\n",
      "step 2100000, average loss = 4.8371\n",
      "step 2110000, average loss = 4.7755\n",
      "step 2120000, average loss = 4.8205\n",
      "step 2130000, average loss = 4.8625\n",
      "step 2140000, average loss = 4.8114\n",
      "step 2150000, average loss = 4.7707\n",
      "step 2160000, average loss = 4.8292\n",
      "step 2170000, average loss = 4.8335\n",
      "step 2180000, average loss = 4.8430\n",
      "step 2190000, average loss = 4.8636\n",
      "step 2200000, average loss = 4.8429\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'one',\n",
      "\"b'of'\" nearest neighbors: b'for', b'within', b'and', b'in', b'group', b'following', b'including', b'while',\n",
      "\"b'going'\" nearest neighbors: b'back', b'just', b'again', b'open', b'least', b'gave', b'smith', b'action',\n",
      "\"b'hardware'\" nearest neighbors: b'computer', b'previous', b'software', b'z', b'operations', b'represent', b'single', b'back',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'english', b'german', b'd', b'national', b'john', b's',\n",
      "\"b'britain'\" nearest neighbors: b'france', b'city', b'empire', b'germany', b'days', b'england', b'country', b'great',\n",
      "step 2210000, average loss = 4.8128\n",
      "step 2220000, average loss = 4.8575\n",
      "step 2230000, average loss = 4.8190\n",
      "step 2240000, average loss = 4.8386\n",
      "step 2250000, average loss = 4.8237\n",
      "step 2260000, average loss = 4.8186\n",
      "step 2270000, average loss = 4.7654\n",
      "step 2280000, average loss = 4.8270\n",
      "step 2290000, average loss = 4.8263\n",
      "step 2300000, average loss = 4.8361\n",
      "step 2310000, average loss = 4.8202\n",
      "step 2320000, average loss = 4.8313\n",
      "step 2330000, average loss = 4.6470\n",
      "step 2340000, average loss = 4.7765\n",
      "step 2350000, average loss = 4.8015\n",
      "step 2360000, average loss = 4.7771\n",
      "step 2370000, average loss = 4.7769\n",
      "step 2380000, average loss = 4.7865\n",
      "step 2390000, average loss = 4.7792\n",
      "step 2400000, average loss = 4.8194\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'nine',\n",
      "\"b'of'\" nearest neighbors: b'following', b'for', b'and', b'within', b'in', b'from', b'while', b'with',\n",
      "\"b'going'\" nearest neighbors: b'back', b'just', b'again', b'least', b'gave', b'open', b'smith', b'action',\n",
      "\"b'hardware'\" nearest neighbors: b'previous', b'computer', b'software', b'z', b'operations', b'represent', b'back', b'value',\n",
      "\"b'american'\" nearest neighbors: b'british', b'english', b'french', b'german', UNK, b'd', b'b', b'john',\n",
      "\"b'britain'\" nearest neighbors: b'france', b'germany', b'england', b'city', b'empire', b'great', b'country', b'days',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2410000, average loss = 4.7044\n",
      "step 2420000, average loss = 4.7895\n",
      "step 2430000, average loss = 4.7952\n",
      "step 2440000, average loss = 4.8243\n",
      "step 2450000, average loss = 4.7992\n",
      "step 2460000, average loss = 4.8165\n",
      "step 2470000, average loss = 4.7753\n",
      "step 2480000, average loss = 4.8052\n",
      "step 2490000, average loss = 4.8171\n",
      "step 2500000, average loss = 4.7747\n",
      "step 2510000, average loss = 4.8119\n",
      "step 2520000, average loss = 4.7716\n",
      "step 2530000, average loss = 4.7684\n",
      "step 2540000, average loss = 4.7589\n",
      "step 2550000, average loss = 4.7839\n",
      "step 2560000, average loss = 4.7986\n",
      "step 2570000, average loss = 4.7927\n",
      "step 2580000, average loss = 4.8047\n",
      "step 2590000, average loss = 4.7930\n",
      "step 2600000, average loss = 4.5722\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'nine', b'zero',\n",
      "\"b'of'\" nearest neighbors: b'following', b'for', b'and', b'from', b'in', b'while', b'under', b'including',\n",
      "\"b'going'\" nearest neighbors: b'back', b'again', b'gave', b'least', b'just', b'smith', b'open', b'action',\n",
      "\"b'hardware'\" nearest neighbors: b'previous', b'computer', b'software', b'represent', b'z', b'operations', b'value', b'back',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'english', b'd', b'german', b'b', b'nine', b'and',\n",
      "\"b'britain'\" nearest neighbors: b'france', b'england', b'germany', b'great', b'city', b'empire', b'country', b'countries',\n",
      "step 2610000, average loss = 4.7591\n",
      "step 2620000, average loss = 4.7828\n",
      "step 2630000, average loss = 4.7496\n",
      "step 2640000, average loss = 4.7246\n",
      "step 2650000, average loss = 4.7482\n",
      "step 2660000, average loss = 4.7833\n",
      "step 2670000, average loss = 4.7428\n",
      "step 2680000, average loss = 4.6980\n",
      "step 2690000, average loss = 4.7488\n",
      "step 2700000, average loss = 4.7715\n",
      "step 2710000, average loss = 4.7768\n",
      "step 2720000, average loss = 4.7989\n",
      "step 2730000, average loss = 4.7675\n",
      "step 2740000, average loss = 4.7386\n",
      "step 2750000, average loss = 4.7883\n",
      "step 2760000, average loss = 4.7545\n",
      "step 2770000, average loss = 4.7704\n",
      "step 2780000, average loss = 4.7659\n",
      "step 2790000, average loss = 4.7445\n",
      "step 2800000, average loss = 4.6947\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'nine',\n",
      "\"b'of'\" nearest neighbors: b'following', b'within', b'for', b'in', b'and', b'from', b'while', b'under',\n",
      "\"b'going'\" nearest neighbors: b'back', b'gave', b'again', b'just', b'least', b'smith', b'open', b'action',\n",
      "\"b'hardware'\" nearest neighbors: b'previous', b'computer', b'software', b'represent', b'value', b'z', b'operations', b'back',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'german', b'english', b'd', b'nine', b'john', b'b',\n",
      "\"b'britain'\" nearest neighbors: b'france', b'england', b'germany', b'great', b'empire', b'city', b'island', b'country',\n",
      "step 2810000, average loss = 4.7743\n",
      "step 2820000, average loss = 4.7542\n",
      "step 2830000, average loss = 4.7780\n",
      "step 2840000, average loss = 4.7538\n",
      "step 2850000, average loss = 4.7667\n",
      "step 2860000, average loss = 4.6254\n",
      "step 2870000, average loss = 4.6755\n",
      "step 2880000, average loss = 4.7550\n",
      "step 2890000, average loss = 4.7105\n",
      "step 2900000, average loss = 4.7276\n",
      "step 2910000, average loss = 4.7069\n",
      "step 2920000, average loss = 4.7159\n",
      "step 2930000, average loss = 4.7770\n",
      "step 2940000, average loss = 4.6538\n",
      "step 2950000, average loss = 4.7248\n",
      "step 2960000, average loss = 4.7462\n",
      "step 2970000, average loss = 4.7462\n",
      "step 2980000, average loss = 4.7485\n",
      "step 2990000, average loss = 4.7514\n",
      "step 3000000, average loss = 4.7607\n",
      "evaluation:\n",
      "\"b'five'\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'zero', b'one',\n",
      "\"b'of'\" nearest neighbors: b'within', b'following', b'for', b'and', b'including', b'with', b'from', b'in',\n",
      "\"b'going'\" nearest neighbors: b'back', b'gave', b'just', b'open', b'smith', b'again', b'least', b'off',\n",
      "\"b'hardware'\" nearest neighbors: b'previous', b'computer', b'software', b'represent', b'z', b'value', b'technical', b'operations',\n",
      "\"b'american'\" nearest neighbors: b'british', b'french', b'german', b'english', b'former', b'national', b'and', b'film',\n",
      "\"b'britain'\" nearest neighbors: b'france', b'england', b'germany', b'empire', b'great', b'city', b'island', b'country',\n"
     ]
    }
   ],
   "source": [
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # run the initializes\n",
    "    sess.run(init)\n",
    "    \n",
    "    # testing data\n",
    "    x_test = np.array([word2id[w] for w in eval_words])\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(1, num_steps + 1):\n",
    "        # get a new batch of data\n",
    "        batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
    "        # run trainingop\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict = {X:batch_x, Y:batch_y})\n",
    "        average_loss += loss\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            if step > 1:\n",
    "                average_loss /= display_step\n",
    "            print('step ' + str(step) + ', average loss = ' + '{:.4f}'.format(average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        # evaluation \n",
    "        if step % eval_step == 0 or step == 1:\n",
    "            print('evaluation:')\n",
    "            sim = sess.run(cosine_sim_op, feed_dict={X:x_test})\n",
    "            for i in range(len(eval_words)):\n",
    "                top_k = 8 # number of nearest neughbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
    "                for k in range(top_k):\n",
    "                    log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
    "                print(log_str)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
