{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import cluster\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    " \n",
    "#KTF.set_session(tf.Session(config=tf.ConfigProto(device_count={'gpu':0})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #（其中0.1是选择所调用的gpu）\n",
    "\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "#指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto() \n",
    "#不全部占满显存, 按需分配\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_svm(file):\n",
    "    encodings = []\n",
    "    labels = []\n",
    "    with open(file) as f:\n",
    "        records = f.readlines()\n",
    "\n",
    "    for line in records:\n",
    "        line = re.sub('\\d+:', '', line)\n",
    "        array = line.strip().split() if line.strip() != '' else None\n",
    "        encodings.append(array[1:])\n",
    "        labels.append(int(array[0]))\n",
    "\n",
    "    return np.array(encodings).astype(float), np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ROC_curve(y_test,y_predict,savepath):\n",
    "    '''\n",
    "    画ROC曲线\n",
    "    '''\n",
    "    false_positive_rate,true_positive_rate,thresholds=roc_curve(y_test, y_predict)\n",
    "    roc_auc=auc(false_positive_rate, true_positive_rate)\n",
    "    plt.title('ROC')\n",
    "    plt.plot(false_positive_rate, true_positive_rate,'b',label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.savefig(savepath)\n",
    "    plt.close(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for a binary classifier\n",
    "def auc_1(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割并序列编码\n",
    "AA = '-GAVLIFWYDNEKQMSTCPHR'\n",
    "def pep(path, seq_len):\n",
    "    seqs = open(path).readlines()\n",
    "    cut = (len(seqs[0].split()[0]) - 1 - seq_len) // 2\n",
    "    X = [[AA.index(res.upper()) if res.upper() in AA else 0\n",
    "          for res in (seq.split()[0][cut:-cut] if cut != 0 else seq.split()[0])]\n",
    "        for seq in seqs if seq.strip() != '']\n",
    "    y = [int(seq.split()[-1]) for seq in seqs if seq.strip() != '']\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'C:/Users/Crow/Desktop/human_data/Step_11_CV/Train.txt'\n",
    "path2 = 'C:/Users/Crow/Desktop/human_data/Step_11_IND/Independent.txt'\n",
    "path_train =  'C:/Users/Crow/Desktop/human_data/Step_11_CV/Train_29_EGAAC_cv.txt'\n",
    "path_train2 = 'C;/Users/Crow/Desktop/human_data/Step_11_CV/Train_29_EGAAC_gap4.txt'\n",
    "path_test =  'C:/Users/Crow/Desktop/human_data/Step_11_IND/Test_29_EGAAC_gap4.txt'\n",
    "\n",
    "\n",
    "plant_train = 'C:/Users/Crow/Desktop/plant_data/Step_11_CV/Train.txt'\n",
    "plant_test = 'C:/Users/Crow/Desktop/plant_data/Step_11_IND/Independent.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plant_train,y_plant_train = pep(plant_train,25)\n",
    "x_plant_test,y_plant_test = pep(plant_test,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_svm(path_train)\n",
    "test = read_svm(path_test)\n",
    "\n",
    "x_train = train[0]\n",
    "y_train = train[1]\n",
    "\n",
    "x_test = test[0]\n",
    "y_test = test[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2,y_train2 = pep(path1,29)\n",
    "x_test2,y_test2 = pep(path2,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN(embed_input_dim, embed_output_dim, lstm_dim, input_length, dropout=0.2, weights=None)\n",
    "model = Sequential()\n",
    "model.add(Embedding(embed_input_dim, embed_output_dim, input_length=input_length, weights=weights, trainable=True))\n",
    "\n",
    "# model.add(GRU(lstm_dim, implementation=2, return_sequences=True))\n",
    "# model.add(Dropout(dropout))\n",
    "\n",
    "model.add(GRU(lstm_dim, implementation=2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 全连接网络\n",
    "model = Sequential()\n",
    "model.add(Dense(130, input_shape=(x_train.shape[1],)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(130))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(130))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(130))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(1)) # 这里需要和输出的维度一致\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "epochs = 100\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=260, shuffle=True,validation_split = 0.2,\n",
    "         callbacks=callbacks_list)\n",
    "\n",
    "pre = model.predict_proba(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一维卷积测试\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(130, 9, strides=1, padding='valid', dilation_rate=1, activation=None, \n",
    "                          use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                          kernel_regularizer=None, bias_regularizer=None, \n",
    "                          activity_regularizer=None, kernel_constraint=None, bias_constraint=None, input_shape=(x_train.shape[0],130)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用LSTM的序列分类\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(130, output_dim=256))\n",
    "model.add(Embedding(22, 64, input_length = 31))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# filepath=\"C:/Users/Crow/Desktop/human_data/LSTM/checkpoint-{val_loss:.2f}-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "model.fit(x_train2, y_train2, batch_size=1024, epochs=25, shuffle=True,validation_split = 0.2,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "#score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "pre = model.predict_proba(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(model.evaluate(x_test2,y_test2))\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "draw_ROC_curve(y_test2,pre,savepath='C:/Users/Crow/Desktop/human_data/LSTM_12.5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in locals()['In']:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(x_train.shape[1],)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(1)) # 这里需要和输出的维度一致\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[auc_1])\n",
    "epochs = 100\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=256, validation_split=0.1, shuffle=True)\n",
    "\n",
    "pre = model.predict_proba(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model.predict_proba(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "neurons=7\n",
    "window=120\n",
    "dropout=0.3\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32, input_length = 31))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(neurons, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filepath=\"C:/Users/Crow/Desktop/human_data/CNN/checkpoint-{val_loss:.2f}-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "\n",
    "# epochs = 9\n",
    "# history = model.fit(x_train2, y_train2, epochs=epochs, batch_size=256, validation_split=0.1, shuffle=True,callbacks=callbacks_list)\n",
    "# pre = model.predict(x_test2)\n",
    "# fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "# print(sklearn.metrics.auc(fpr, tpr))\n",
    "# draw_ROC_curve(y_test2,pre,savepath='C:/Users/Crow/Desktop/human_data/CNN_11.28.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train2, y_train2,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 256,\n",
    "                   shuffle=True,validation_split = 0.2,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "print(model.evaluate(x_test2, y_test2, batch_size=256))\n",
    "pre = model.predict(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "draw_ROC_curve(y_test2,pre,savepath='C:/Users/Crow/Desktop/human_data/CNN_11.30.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre2 = model.predict(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre2,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(28,28,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[auc_1])\n",
    "epochs = 200\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=1000, validation_split=0.1, shuffle=True)\n",
    "\n",
    "pre = model.predict_proba(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"99\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一维卷积测试\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(x_train2.shape[0], 64, input_length = 31))\n",
    "\n",
    "model.add(Conv1D(130, 9, strides=1, padding='valid', dilation_rate=1, activation=None, \n",
    "                          use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                          kernel_regularizer=None, bias_regularizer=None, \n",
    "                          activity_regularizer=None, kernel_constraint=None, bias_constraint=None, input_shape=(x_train.shape[0],31)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[auc_1])\n",
    "\n",
    "model.fit(x_train2, y_train2, batch_size=200, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window  = 7\n",
    "neurons = 120\n",
    "dropout= 0.3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length = 31))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(neurons, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "    \n",
    "history = model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath=\"C:/Users/Crow/Desktop/human_data/CNN/checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "model.fit(x_train2, y_train2,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 1024,\n",
    "                   shuffle=True,validation_split = 0.2,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "print(model.evaluate(x_test2, y_test2, batch_size=256))\n",
    "pre = model.predict(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "draw_ROC_curve(y_test2,pre,savepath='C:/Users/Crow/Desktop/human_data/CNN_11.30.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/Users/Crow/Desktop/human_data/human_CNN_w7_n_120_d_0.3_l_31_0.866.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('C:/Users/Crow/Desktop/human_data/human_CNN_w7_n_120_d_0.2_l_27.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model2.predict(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plant\n",
    "\n",
    "x_plant_train,y_plant_train = pep(plant_train,29)\n",
    "x_plant_test,y_plant_test = pep(plant_test,29)\n",
    "\n",
    "\n",
    "window  = 7\n",
    "neurons = 100\n",
    "dropout= 0.3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length = 31))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(neurons, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath=\"C:/Users/Crow/Desktop/plant_data/CNN/checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "history = model.fit(x_plant_train, y_plant_train,\n",
    "                   epochs = 15,\n",
    "                   batch_size = 500,\n",
    "                   validation_split = 0.2,shuffle=True,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "print(model.evaluate(x_plant_test, y_plant_test, batch_size=256))\n",
    "\n",
    "pre = model.predict(x_plant_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_plant_test,pre,pos_label=1)\n",
    "print(sklearn.metrics. auc(fpr, tpr))\n",
    "\n",
    "\n",
    "draw_ROC_curve(y_plant_test,pre,savepath='C:/Users/Crow/Desktop/plant_data/plant_CNN_12.4.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/Users/Crow/Desktop/plant_data/plant_CNN_w7_n_100_d_0.3_l_29_0.83.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plant\n",
    "#使用LSTM的序列分类\n",
    "x_plant_train,y_plant_train = pep(plant_train,23)\n",
    "x_plant_test,y_plant_test = pep(plant_test,23)\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(130, output_dim=256))\n",
    "model.add(Embedding(1000, 32, input_length = 25))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy',auc_1])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filepath=\"C:/Users/Crow/Desktop/plant_data/LSTM/checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(x_plant_train, y_plant_train,\n",
    "                   epochs = 35,\n",
    "                   batch_size = 256,\n",
    "                   validation_split = 0.2,shuffle=True,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "pre = model.predict_proba(x_plant_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_plant_test,pre,pos_label=1)\n",
    "print(model.evaluate(x_plant_test,y_plant_test))\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "draw_ROC_curve(y_plant_test,pre,savepath='C:/Users/Crow/Desktop/plant_data/LSTM_plant_11_28_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['auc_1'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['auc_1'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_human_cnn():\n",
    "    neurons = 120\n",
    "    dropout= 0.3\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(1000, 64, input_length = 31))\n",
    "\n",
    "    model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 120\n",
    "dropout= 0.3\n",
    "filepath=\"C:/Users/Crow/Desktop/human_data/CNN/checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "model.fit(x_train2, y_train2,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 1024,\n",
    "                   shuffle=True,validation_split = 0.2,\n",
    "         callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold \n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "\n",
    "for train_index, test_index in kf.split(x_train2):\n",
    "\n",
    "    x_train3, x_test3 = x_train2[train_index], x_train2[test_index]\n",
    "    y_train3, y_test3 = y_train2[train_index], y_train2[test_index]\n",
    "    \n",
    "    custom_resnet = create_human_cnn()\n",
    "    #filepath=\"C:/Users/Crow/Desktop/human_data/CNN/checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "    #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    callbacks_list = [early_stopping]\n",
    "\n",
    "    custom_resnet.fit(x_train3, y_train3, validation_data = (x_test3, y_test3), epochs = 20, batch_size = 1024,shuffle=True,\n",
    "         callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_resnet.evaluate(x_test2, y_test2, batch_size=256))\n",
    "pre = custom_resnet.predict(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "draw_ROC_curve(y_test2,pre,savepath='C:/Users/Crow/Desktop/human_data/CNN_k-fold_12.4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "\n",
    "for train_index, test_index in kf.split(x_plant_train):\n",
    "\n",
    "    x_train3, x_test3 = x_plant_train[train_index], x_plant_train[test_index]\n",
    "    y_train3, y_test3 = y_plant_train[train_index], y_plant_train[test_index]\n",
    "    \n",
    "    custom_resnet = create_human_cnn()\n",
    "    #filepath=\"C:/Users/Crow/Desktop/human_data/CNN/checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "    #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    callbacks_list = [early_stopping]\n",
    "\n",
    "    custom_resnet.fit(x_train3, y_train3, validation_data = (x_test3, y_test3), epochs = 20, batch_size = 256,shuffle=True,\n",
    "         callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_resnet.evaluate(x_plant_test, y_plant_test, batch_size=256))\n",
    "pre = custom_resnet.predict(x_plant_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_plant_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "draw_ROC_curve(y_plant_test,pre,savepath='C:/Users/Crow/Desktop/plant_data/CNN_10-fold.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手写CRF\n",
    "\n",
    "class CRF(Layer):\n",
    "    \"\"\"纯Keras实现CRF层\n",
    "    CRF层本质上是一个带训练参数的loss计算层，因此CRF层只用来训练模型，\n",
    "    而预测则需要另外建立模型。\n",
    "    \"\"\"\n",
    "    def __init__(self, ignore_last_label=False, **kwargs):\n",
    "        \"\"\"ignore_last_label：定义要不要忽略最后一个标签，起到mask的效果\n",
    "        \"\"\"\n",
    "        self.ignore_last_label = 1 if ignore_last_label else 0\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.num_labels = input_shape[-1] - self.ignore_last_label\n",
    "        self.trans = self.add_weight(name='crf_trans',\n",
    "                                     shape=(self.num_labels, self.num_labels),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "    def log_norm_step(self, inputs, states):\n",
    "        \"\"\"递归计算归一化因子\n",
    "        要点：1、递归计算；2、用logsumexp避免溢出。\n",
    "        技巧：通过expand_dims来对齐张量。\n",
    "        \"\"\"\n",
    "        states = K.expand_dims(states[0], 2) # (batch_size, output_dim, 1)\n",
    "        trans = K.expand_dims(self.trans, 0) # (1, output_dim, output_dim)\n",
    "        output = K.logsumexp(states+trans, 1) # (batch_size, output_dim)\n",
    "        return output+inputs, [output+inputs]\n",
    "    def path_score(self, inputs, labels):\n",
    "        \"\"\"计算目标路径的相对概率（还没有归一化）\n",
    "        要点：逐标签得分，加上转移概率得分。\n",
    "        技巧：用“预测”点乘“目标”的方法抽取出目标路径的得分。\n",
    "        \"\"\"\n",
    "        point_score = K.sum(K.sum(inputs*labels, 2), 1, keepdims=True) # 逐标签得分\n",
    "        labels1 = K.expand_dims(labels[:, :-1], 3)\n",
    "        labels2 = K.expand_dims(labels[:, 1:], 2)\n",
    "        labels = labels1 * labels2 # 两个错位labels，负责从转移矩阵中抽取目标转移得分\n",
    "        trans = K.expand_dims(K.expand_dims(self.trans, 0), 0)\n",
    "        trans_score = K.sum(K.sum(trans*labels, [2,3]), 1, keepdims=True)\n",
    "        return point_score+trans_score # 两部分得分之和\n",
    "    def call(self, inputs): # CRF本身不改变输出，它只是一个loss\n",
    "        return inputs\n",
    "    def loss(self, y_true, y_pred): # 目标y_pred需要是one hot形式\n",
    "        mask = 1-y_true[:,1:,-1] if self.ignore_last_label else None\n",
    "        y_true,y_pred = y_true[:,:,:self.num_labels],y_pred[:,:,:self.num_labels]\n",
    "        init_states = [y_pred[:,0]] # 初始状态\n",
    "        log_norm,_,_ = K.rnn(self.log_norm_step, y_pred[:,1:], init_states, mask=mask) # 计算Z向量（对数）\n",
    "        log_norm = K.logsumexp(log_norm, 1, keepdims=True) # 计算Z（对数）\n",
    "        path_score = self.path_score(y_pred, y_true) # 计算分子（对数）\n",
    "        return log_norm - path_score # 即log(分子/分母)\n",
    "    def accuracy(self, y_true, y_pred): # 训练过程中显示逐帧准确率的函数，排除了mask的影响\n",
    "        mask = 1-y_true[:,:,-1] if self.ignore_last_label else None\n",
    "        y_true,y_pred = y_true[:,:,:self.num_labels],y_pred[:,:,:self.num_labels]\n",
    "        isequal = K.equal(K.argmax(y_true, 2), K.argmax(y_pred, 2))\n",
    "        isequal = K.cast(isequal, 'float32')\n",
    "        if mask == None:\n",
    "            return K.mean(isequal)\n",
    "        else:\n",
    "            return K.sum(isequal*mask) / K.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_contrib Bi-LSTM+CRF \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras_contrib.layers.crf import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 2500\n",
    "EMBEDDING_OUT_DIM = 128\n",
    "TIME_STAMPS = 31\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.3\n",
    "NUM_CLASS = 5\n",
    "\n",
    "\n",
    "def build_embedding_bilstm2_crf_model():\n",
    "    \"\"\"\n",
    "    带embedding的双向LSTM + crf\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, output_dim=EMBEDDING_OUT_DIM, input_length=TIME_STAMPS))\n",
    "    model.add(Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True)))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True)))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(TimeDistributed(Dense(NUM_CLASS)))\n",
    "    crf_layer = CRF(NUM_CLASS)\n",
    "    model.add(crf_layer)\n",
    "    model.compile('rmsprop', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])\n",
    "    return model\n",
    "\n",
    "def save_embedding_bilstm2_crf_model(model, filename):\n",
    "    save_load_utils.save_all_weights(model,filename)\n",
    "\n",
    "def load_embedding_bilstm2_crf_model(filename):\n",
    "    model = build_embedding_bilstm2_crf_model()\n",
    "    save_load_utils.load_all_weights(model, filename)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = build_embedding_bilstm2_crf_model()\n",
    "model.fit(x_train2, y_train2, batch_size=256, epochs=25, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "#score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "pre = model.predict_proba(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(model.evaluate(x_test2,y_test2))\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "input = Input(shape=(31,))\n",
    "model = Embedding(input_dim=22, output_dim=20,\n",
    "                  input_length=31, mask_zero=True)(input)  # 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(2)  # CRF layer\n",
    "out = crf(model)  # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_test2, y_test2, batch_size=32, epochs=5, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(23, EMBEDDING_SIZE,input_length=31))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "history = model.fit(x_test2, y_test2, batch_size=32, epochs=25, validation_split=0.2, shuffle=True,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "pre = model.predict_proba(x_test2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test2,pre,pos_label=1)\n",
    "print(model.evaluate(x_test2,y_test2))\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'C:/Users/Crow/Desktop/human_data_12.12/Train.txt'\n",
    "path_test =  'C:/Users/Crow/Desktop/human_data_12.12/Independent.txt'\n",
    "\n",
    "x_train,y_train = pep(path_train,29)\n",
    "x_test,y_test = pep(path_test,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73793, 29)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59034 samples, validate on 14759 samples\n",
      "Epoch 1/20\n",
      "59034/59034 [==============================] - 4s 65us/step - loss: 0.3523 - acc: 0.8865 - val_loss: 0.2307 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.3154 - acc: 0.8867 - val_loss: 0.1178 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.2913 - acc: 0.8867 - val_loss: 0.1543 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.2835 - acc: 0.8867 - val_loss: 0.1468 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.2769 - acc: 0.8867 - val_loss: 0.1015 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.2699 - acc: 0.8867 - val_loss: 0.1192 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.2697 - acc: 0.8869 - val_loss: 0.1225 - val_acc: 0.9998\n",
      "Epoch 8/20\n",
      "59034/59034 [==============================] - 3s 54us/step - loss: 0.2656 - acc: 0.8876 - val_loss: 0.1289 - val_acc: 0.9957\n",
      "Epoch 9/20\n",
      "59034/59034 [==============================] - 3s 53us/step - loss: 0.2662 - acc: 0.8871 - val_loss: 0.1452 - val_acc: 0.9933\n",
      "17980/17980 [==============================] - 0s 9us/step\n",
      "[0.2325415920859847, 0.9155172413793103]\n",
      "0.8603777690878307\n"
     ]
    }
   ],
   "source": [
    "# CNN test\n",
    "\n",
    "neurons=128\n",
    "window=5\n",
    "dropout=0.4\n",
    "model = Sequential()\n",
    "model.add(Embedding(21, 5, input_length = 31))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Conv1D(neurons, window, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(neurons, activation='relu'))\n",
    "# model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filepath=\"C:/Users/Crow/Desktop/human_data/CNN/checkpoint-{val_loss:.2f}-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5\"\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max', period=4)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 128,\n",
    "                   shuffle=True,validation_split = 0.2,\n",
    "         callbacks=callbacks_list, verbose=1)\n",
    "print(model.evaluate(x_test, y_test, batch_size=256))\n",
    "pre = model.predict(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))\n",
    "#draw_ROC_curve(y_test2,pre,savepath='C:/Users/Crow/Desktop/human_data/CNN_12.5.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'D:/Graphviz2.38/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='keras_test_model1.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-23-1aa69cc6b2f5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-1aa69cc6b2f5>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    model = load_model('C:\\Users\\Crow\\Desktop\\human_data\\CNN\\128_8_0.4_3conv1d_12.5.hdf5')\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# 读取model h5文件\n",
    "model = load_model('C:\\Users\\Crow\\Desktop\\human_data\\CNN\\128_8_0.4_3conv1d_12.5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model('c:/Users/Crow/Desktop/human_data/CNN/128_8_0.4_3conv1d_12.5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_183 (Embedding)    (None, 29, 32)            704       \n",
      "_________________________________________________________________\n",
      "conv1d_559 (Conv1D)          (None, 29, 128)           32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_559 (MaxPoolin (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_731 (Dropout)        (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_560 (Conv1D)          (None, 14, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_560 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_732 (Dropout)        (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_561 (Conv1D)          (None, 7, 128)            131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_561 (MaxPoolin (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_733 (Dropout)        (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_173 (Flatten)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 1)                 385       \n",
      "=================================================================\n",
      "Total params: 296,385\n",
      "Trainable params: 296,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17980/17980 [==============================] - 0s 10us/step\n",
      "[0.24421920618570686, 0.9059510567429598]\n",
      "0.8653889378771376\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train = pep(path_train,27)\n",
    "x_test,y_test = pep(path_test,27)\n",
    "print(model.evaluate(x_test, y_test, batch_size=256))\n",
    "pre = model.predict(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pre,pos_label=1)\n",
    "print(sklearn.metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
